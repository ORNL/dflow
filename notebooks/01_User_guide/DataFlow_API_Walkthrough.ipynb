{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFlow API walkthrough\n",
    "Suhas Somnath <br>\n",
    "4/6/2022 <br>\n",
    "Oak Ridge National Laboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from collections.abc import MutableMapping\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "class Transport(Enum):\n",
    "    \"\"\"\n",
    "    The different data transfer protocols supported by DataFlow\n",
    "    \"\"\"\n",
    "    GLOBUS = 0\n",
    "    HTTPS = 1\n",
    "\n",
    "\n",
    "class API(object):\n",
    "\n",
    "    def __init__(self, api_key, server_url=None):\n",
    "        \"\"\"\n",
    "        Creates an instance of the API class to communicate with DataFlow\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        api_key : str\n",
    "            API key for accessing DataFlow\n",
    "        server_url : str, Optional\n",
    "            URL for DataFlow server.\n",
    "            Default: staging server\n",
    "        \"\"\"\n",
    "        if not isinstance(api_key, str):\n",
    "            raise TypeError(\"api_key should be a string. Generate this from DataFlow\")\n",
    "        if server_url:\n",
    "            if not isinstance(server_url, str):\n",
    "                raise TypeError(\"server_url should be a str\")\n",
    "            self._API_URL = server_url\n",
    "        else:\n",
    "            print(\"Using staging server as default\")\n",
    "            self._API_URL = \"https://dataflow-staging.ornl.gov/api/v1\"\n",
    "        self._API_KEY = api_key\n",
    "\n",
    "    def __get(self, url):\n",
    "        \"\"\"\n",
    "        Internal function to send GET requests\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url : str\n",
    "            URL for GET request\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response to GET request\n",
    "        \"\"\"\n",
    "        headers = {\"accept\": \"*/*\",\n",
    "                   \"Authorization\": self._API_KEY}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if not response.ok:\n",
    "            raise ValueError(\"{}: {}\".format(response.reason, response.text[1:-1]))\n",
    "        return response.json()\n",
    "\n",
    "    def __post(self, url, headers={}, json=None, data=None, files=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url : str\n",
    "            URL for POST request\n",
    "        headers : dict, optional\n",
    "            Headers besides the accept and auth token\n",
    "        json : dict, optional\n",
    "            Dict\n",
    "        data : dict, optional\n",
    "            Key-value pairs for the form\n",
    "        files : dict\n",
    "            Dict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response to POST request\n",
    "        \"\"\"\n",
    "        # TODO: Use **kwargs instead\n",
    "        basic_headers = {\"accept\": \"*/*\",\n",
    "                         \"Authorization\": self._API_KEY}\n",
    "        basic_headers.update(headers)\n",
    "        response = requests.post(url,\n",
    "                                 headers=basic_headers,\n",
    "                                 json=json, files=files,\n",
    "                                 data=data)\n",
    "        if not response.ok:\n",
    "            raise ValueError(\"{}: {}\".format(response.reason, response.text[1:-1]))\n",
    "        return response.json()\n",
    "\n",
    "    @staticmethod\n",
    "    def __validate_integer(value, title, min_val=0):\n",
    "        \"\"\"\n",
    "        Validates integer parameter\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : int\n",
    "            value to validate\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "        ValueError\n",
    "        \"\"\"\n",
    "        if not isinstance(value, int):\n",
    "            raise TypeError(\"{} should be an int\".format(title))\n",
    "        if value < min_val:\n",
    "            raise ValueError(\"{} should be > {}\".format(title, min_val))\n",
    "\n",
    "    @staticmethod\n",
    "    def __validate_str_parm(value, title):\n",
    "        \"\"\"\n",
    "        Validate string parameter\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : str\n",
    "            Object to test\n",
    "        title : str\n",
    "            Name of variable or parameter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "        ValueError\n",
    "        \"\"\"\n",
    "        mesg = '{} should be a non empty string'.format(title)\n",
    "        if not isinstance(value, str):\n",
    "            raise TypeError(mesg)\n",
    "        title = title.strip()\n",
    "        if len(title) < 1:\n",
    "            raise ValueError(mesg)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __flatten_dict(nested_dict, separator='-'):\n",
    "        \"\"\"\n",
    "        Flattens a nested dictionary\n",
    "        Parameters\n",
    "        ----------\n",
    "        nested_dict : dict\n",
    "            Nested dictionary\n",
    "        separator : str, Optional. Default='-'\n",
    "            Separator between the keys of different levels\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary whose keys are flattened to a single level\n",
    "        Notes\n",
    "        -----\n",
    "        Taken from https://stackoverflow.com/questions/6027558/flatten-nested-\n",
    "        dictionaries-compressing-keys\n",
    "        \"\"\"\n",
    "        if not isinstance(nested_dict, dict):\n",
    "            raise TypeError('nested_dict should be a dict')\n",
    "\n",
    "        def __flatten_dict_int(nest_dict, sep, parent_key=''):\n",
    "            items = []\n",
    "            if sep == '_':\n",
    "                repl = '-'\n",
    "            else:\n",
    "                repl = '_'\n",
    "            for key, value in nest_dict.items():\n",
    "                if not isinstance(key, str):\n",
    "                    key = str(key)\n",
    "                if sep in key:\n",
    "                    key = key.replace(sep, repl)\n",
    "\n",
    "                new_key = parent_key + sep + key if parent_key else key\n",
    "                if isinstance(value, MutableMapping):\n",
    "                    items.extend(__flatten_dict_int(value, sep, parent_key=new_key).items())\n",
    "                # nion files contain lists of dictionaries, oops\n",
    "                elif isinstance(value, list):\n",
    "                    for i in range(len(value)):\n",
    "                        if isinstance(value[i], dict):\n",
    "                            for kk in value[i]:\n",
    "                                items.append(('dim-' + kk + '-' + str(i), value[i][kk]))\n",
    "                        else:\n",
    "                            if type(value) != bytes:\n",
    "                                items.append((new_key, value))\n",
    "                else:\n",
    "                    if type(value) != bytes:\n",
    "                        items.append((new_key, value))\n",
    "            return dict(items)\n",
    "\n",
    "        return __flatten_dict_int(nested_dict, separator)\n",
    "            \n",
    "    def settings_get(self):\n",
    "        \"\"\"\n",
    "        Gets current default user settings\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response from GET request\n",
    "        \"\"\"\n",
    "        path = \"user-settings\"\n",
    "        url = \"%s/%s\" % (self._API_URL, path)\n",
    "        return self.__get(url)\n",
    "    \n",
    "    def settings_set(self, setting, value):\n",
    "        \"\"\"\n",
    "        Set or update default user settings\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        setting : str\n",
    "            Name of parameter\n",
    "            Currently, only \"globus.destination_endpoint\" and \"transport.protocol\" are supported\n",
    "        value : obj\n",
    "            New value for chosen parameter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response from POST request\n",
    "        \"\"\"\n",
    "        self.__validate_str_parm(setting, \"setting\")\n",
    "        path = \"user-settings/?setting={}&value={}\".format(setting, value)\n",
    "        url = \"%s/%s\" % (self._API_URL, path)\n",
    "        return self.__post(url)\n",
    "            \n",
    "    def instrument_list(self):\n",
    "        \"\"\"\n",
    "        List all instruments connected to this DataFlow server\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response from GET request\n",
    "        \"\"\"\n",
    "        url = \"%s/%s\" % (self._API_URL, \"instruments\")\n",
    "        return self.__get(url)\n",
    "    \n",
    "    def instrument_info(self, instr_id):\n",
    "        \"\"\"\n",
    "        Show information about an Instrument\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        instr_id : int\n",
    "            ID for Insrtument\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response from GET request\n",
    "        \"\"\"\n",
    "        self.__validate_integer(instr_id, \"instr_id\", min_val=0)\n",
    "        path = 'instruments/{}'.format(instr_id)\n",
    "        url = \"%s/%s\" % (self._API_URL, path)\n",
    "        return self.__get(url)\n",
    "\n",
    "    def globus_endpoints_active(self, endpoint=None):\n",
    "        \"\"\"\n",
    "        Checks whether both source and destination Globus endpoints are active\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        endpoint : str, Optional. \n",
    "            UUID of endpoint whose status needs to be checked. \n",
    "            Default = None - checks the default destination Globus endpoint \n",
    "            along with the DataFlow server's endpoint\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response from GET request\n",
    "        \"\"\"\n",
    "        url = \"%s/%s\" % (self._API_URL, 'transports/globus/activation')\n",
    "        if isinstance(endpoint, str):\n",
    "            url += \"?endpoint=\" + endpoint\n",
    "        # TODO: What should this response look like to be pythonic?\n",
    "        return self.__get(url)\n",
    "\n",
    "    def globus_endpoints_activate(self, username, password, encrypted=True, endpoint=\"destination\"):\n",
    "        \"\"\"\n",
    "        Activates Globus endpoints necessary to transfer data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        username : str\n",
    "            user name\n",
    "        password : str\n",
    "            password\n",
    "        encrypted : bool, Optional\n",
    "            Whether or not the password is encrypted (using the DataFlow web server's encryption).\n",
    "            Default = encrypted password\n",
    "        endpoint : str, Optional\n",
    "            Endpoint to activate. \n",
    "            The 3 valid options are:\n",
    "            1. \"source\" - DataFlow server's endpoint, \n",
    "            2. \"destination\" - Where data will be sent to, \n",
    "            and 3. the UUID of some other endpoint\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response from GET request\n",
    "        \"\"\"\n",
    "        pwd_prefix = \"encrypted\"\n",
    "        if not encrypted:\n",
    "            pwd_prefix = \"unencrypted\"\n",
    "        path = 'transports/globus/activate?endpoint={}&username={}&{}_password={}'.format(endpoint, username, pwd_prefix, password)\n",
    "        url = \"%s/%s\" % (self._API_URL, path)\n",
    "        return self.__post(url)\n",
    "\n",
    "    def dataset_search(self, query):\n",
    "        \"\"\"\n",
    "        Search for a dataset in DataFlow\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query : str\n",
    "            Text or date to search on\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response from GET request\n",
    "        \"\"\"\n",
    "        self.__validate_str_parm(query, \"query\")\n",
    "        # TODO: Verify that spaces and other non alphanumeric characters are correctly encoded\n",
    "        path = 'datasets/search?q={}'.format(query)\n",
    "        url = \"%s/%s\" % (self._API_URL, path)\n",
    "        return self.__get(url)\n",
    "\n",
    "    def dataset_info(self, dset_id):\n",
    "        \"\"\"\n",
    "        Show information about a dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dset_id : int\n",
    "            ID for dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response from GET request\n",
    "        \"\"\"\n",
    "        self.__validate_integer(dset_id, \"dset_id\", min_val=0)\n",
    "        path = 'datasets/{}'.format(dset_id)\n",
    "        url = \"%s/%s\" % (self._API_URL, path)\n",
    "        return self.__get(url)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __mdata_dict_2_list(metadata):\n",
    "        \"\"\"\n",
    "        Converts metadata dictionary to a list amenable to DataFlow\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        metadata: dict\n",
    "            Metadata specified as {key_1: value_1, key_2: value_2}\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            Metadata reformatted as:\n",
    "            [{\"field_name\": key_1, \"field_value\": value_1}, \n",
    "             {\"field_name\": key_2, \"field_value\": value_2}]\n",
    "        \"\"\"\n",
    "        mdlist = list()\n",
    "        for key, val in metadata.items():\n",
    "            mdlist.append({\"field_name\": key, \"field_value\": val})\n",
    "        return mdlist\n",
    "\n",
    "    def dataset_create(self, title, instrument_id=0, metadata=None):\n",
    "        \"\"\"\n",
    "        Create a new dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        title : str\n",
    "            Title for dataset\n",
    "        instrument_id : int, optional\n",
    "            Instrument ID. Default = 0 - UnknownInstrument\n",
    "        metadata : dict, optional\n",
    "            Scientific metadata associated with this dataset.\n",
    "            Metadata specified as {\"param 1\": value_1, \"param 2\": value_2}\n",
    "            Nested dictionaries will be flattened with keys joined with a \"-\" separator\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response from POST request\n",
    "        \"\"\"\n",
    "        self.__validate_str_parm(title, \"title\")\n",
    "        if metadata:\n",
    "            if not isinstance(metadata, dict):\n",
    "                raise TypeError(\"metadata should be a dict\")\n",
    "            \n",
    "        url = \"%s/%s\" % (self._API_URL, \"datasets\")\n",
    "        data = {\"name\": title,\n",
    "                \"instrument_id\": instrument_id}\n",
    "        if isinstance(metadata, dict):\n",
    "            flat_md = self.__flatten_dict(metadata)\n",
    "            mdata_list = self.__mdata_dict_2_list(flat_md)\n",
    "            data[\"metadata_field_values_attributes\"] = mdata_list\n",
    "\n",
    "        return self.__post(url,\n",
    "                           headers={\"Content-Type\": \"application/json\"},\n",
    "                           json=data)\n",
    "\n",
    "    def files_search(self, query, dataset_id=None):\n",
    "        \"\"\"\n",
    "        Search for individual files in datasets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query : str\n",
    "            Search query\n",
    "        dataset_id : int, optional\n",
    "            Filter results to the specified Dataset. Default - no filtering\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response from GET request\n",
    "        \"\"\"\n",
    "        path = 'dataset-files/search?q={}'.format(query)\n",
    "        if dataset_id is not None:\n",
    "            self.__validate_integer(dataset_id, \"dataset_id\", min_val=0)\n",
    "            path += \"&dataset_id={}\".format(dataset_id)\n",
    "        url = \"%s/%s\" % (self._API_URL, path)\n",
    "        return self.__get(url)\n",
    "\n",
    "    def file_upload(self, file_path, dataset_id, relative_path=None, transport=None):\n",
    "        \"\"\"\n",
    "        Upload the provided file to the specified Dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "            Local path to file that needs to be uploaded\n",
    "        dataset_id : int\n",
    "            Dataset ID to upload this file to\n",
    "        relative_path : str, optional\n",
    "            Relative path in destination to place this file.\n",
    "            Default - the file will be uploaded to the root directory of the dataset\n",
    "        transport : dflow.Transport, optional\n",
    "            Transport protocol to use to transfer this specific file\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Response from POST request\n",
    "        \"\"\"\n",
    "        path = 'dataset-file-upload'\n",
    "        url = \"%s/%s\" % (self._API_URL, path)\n",
    "\n",
    "        self.__validate_str_parm(file_path, \"file_path\")\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(\"{} not found\".format(file_path))\n",
    "\n",
    "        self.__validate_integer(dataset_id, \"dataset_id\", min_val=0)\n",
    "\n",
    "        if transport:\n",
    "            if not isinstance(transport, Transport):\n",
    "                raise TypeError(\"transport should be of type dflow.Transport\")\n",
    "\n",
    "        file_handle = open(file_path, \"rb\")\n",
    "\n",
    "        form_data = {'dataset_id': dataset_id,\n",
    "                     'transport': 'globus'}\n",
    "\n",
    "        if relative_path:\n",
    "            if not isinstance(relative_path, str):\n",
    "                raise TypeError(\"relative_path should be a string\")\n",
    "            form_data.update({'relative_path': relative_path})\n",
    "\n",
    "        if transport != Transport.GLOBUS:\n",
    "            print(\"using Globus since other file transfer adapters have not been implemented\")\n",
    "\n",
    "        response = self.__post(url,\n",
    "                               files={'file': file_handle},\n",
    "                               data=form_data)\n",
    "\n",
    "        file_handle.close()\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prepare to use DataFlow's API:\n",
    "\n",
    "1. Generate an API Key from DataFlow's web interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjoyLCJleHAiOjE2ODIzODA4MDB9.U8QU3a9_b9z879d_iIo9e37Whopkqp9Ha08Gyu0Ep58\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Encrypt password(s) necessary to activate Globus endpoints securely\n",
    "\n",
    "Here, the two Globus endpoints (DataFlow server and destination) use the same authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_pwd = \"8yEtYvktC7RVjIz2o1EghgEZ--vpF6/dv7pkvXZwNV--suXOtctdkvVPnjrBUQoNEg==\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Import the ``API`` class from the ``dflow`` package.\n",
    "\n",
    "Note that the ``dflow`` package is not yet available via PyPi as of this writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dflow import API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the API object with your personal API Key:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using staging server as default\n"
     ]
    }
   ],
   "source": [
    "api = API(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'globus': {'destination_endpoint': '57230a10-7ba2-11e7-8c3b-22000b9923ef'},\n",
       " 'transport': {'protocol': 'globus'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.settings_get()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Update a default setting\n",
    "\n",
    "Here, we will switch the destination endpoint to ``olcf#dtn`` for illustration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'globus': {'destination_endpoint': 'ef1a9560-7ca1-11e5-992c-22000b96db58'},\n",
       " 'transport': {'protocol': 'globus'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.settings_set(\"globus.destination_endpoint\", \n",
    "                            \"ef1a9560-7ca1-11e5-992c-22000b96db58\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching back the destination endpoint to ``cades#CADES-OR`` which is the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'globus': {'destination_endpoint': '57230a10-7ba2-11e7-8c3b-22000b9923ef'},\n",
       " 'transport': {'protocol': 'globus'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.settings_set(\"globus.destination_endpoint\", \n",
    "                            \"57230a10-7ba2-11e7-8c3b-22000b9923ef\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. List and view registered instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'name': 'Cell Cycler',\n",
       "  'description': 'Coin cell Cycler with environment chamber',\n",
       "  'instrument_type': None}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.instrument_list()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'name': 'Cell Cycler',\n",
       " 'description': 'Coin cell Cycler with environment chamber',\n",
       " 'instrument_type': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.instrument_info(1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check to see if Globus endpoints are active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_activation': {'code': 'AlreadyActivated'},\n",
       " 'destination_activation': {'code': 'AutoActivated.CachedCredential'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.globus_endpoints_active(\"57230a10-7ba2-11e7-8c3b-22000b9923ef\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Activate one or both endpoints as necessary:\n",
    "Because the destination wasn't already activated, we can activate that specific endpoint. \n",
    "\n",
    "**Note**: An encrypted password is being used in place of the conventional password for safety reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.globus_endpoints_activate(\"syz\", \n",
    "                                         enc_pwd, \n",
    "                                         encrypted=True, \n",
    "                                         endpoint=\"destination\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_activation': {'code': 'AlreadyActivated'},\n",
       " 'destination_activation': {'code': 'AlreadyActivated'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.globus_endpoints_active()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create a measurement Dataset\n",
    "This creates a directory at the destination Globus Endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 19,\n",
       " 'name': 'My new dataset with nested metadata',\n",
       " 'creator': {'id': 2, 'name': 'Suhas Somnath'},\n",
       " 'dataset_files': [],\n",
       " 'instrument': None,\n",
       " 'metadata_field_values': [{'id': 15,\n",
       "   'field_value': 'PZT',\n",
       "   'field_name': 'Sample',\n",
       "   'metadata_field': None},\n",
       "  {'id': 16,\n",
       "   'field_value': 'Asylum Research',\n",
       "   'field_name': 'Microscope-Vendor',\n",
       "   'metadata_field': None},\n",
       "  {'id': 17,\n",
       "   'field_value': 'MFP3D',\n",
       "   'field_name': 'Microscope-Model',\n",
       "   'metadata_field': None},\n",
       "  {'id': 18,\n",
       "   'field_value': '373',\n",
       "   'field_name': 'Temperature',\n",
       "   'metadata_field': None}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.dataset_create(\"My new dataset with nested metadata\",\n",
    "                               metadata={\"Sample\": \"PZT\", \n",
    "                                         \"Microscope\": {\n",
    "                                             \"Vendor\": \"Asylum Research\",\n",
    "                                             \"Model\": \"MFP3D\"\n",
    "                                             },\n",
    "                                         \"Temperature\": 373\n",
    "                                        }\n",
    "                              )\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the dataset ID programmatically to use later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id = response['id']\n",
    "dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload data file(s) to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Globus since other file transfer adapters have not been implemented\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 50,\n",
       " 'name': 'AFM_Topography.PNG',\n",
       " 'file_length': 201287,\n",
       " 'file_type': '',\n",
       " 'created_at': '2022-04-25 22:33:53 UTC',\n",
       " 'relative_path': '',\n",
       " 'is_directory': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.file_upload(\"./AFM_Topography.PNG\", dataset_id)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload another data file to the same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Globus since other file transfer adapters have not been implemented\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 51,\n",
       " 'name': 'measurement_configuration.txt',\n",
       " 'file_length': 1172,\n",
       " 'file_type': '',\n",
       " 'created_at': '2022-04-25 22:34:01 UTC',\n",
       " 'relative_path': 'foo/bar',\n",
       " 'is_directory': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.file_upload(\"./measurement_configuration.txt\", dataset_id, relative_path=\"foo/bar\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Search Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 2,\n",
       " 'has_more': False,\n",
       " 'results': [{'id': 18,\n",
       "   'created_at': '2022-04-25T22:12:47Z',\n",
       "   'name': 'Dataset with nested metadata 1',\n",
       "   'dataset_files': [],\n",
       "   'metadata_field_values': [{'id': 12,\n",
       "     'field_value': 'PZT',\n",
       "     'field_name': 'Sample',\n",
       "     'metadata_field': None},\n",
       "    {'id': 13,\n",
       "     'field_value': 'Asylum Research',\n",
       "     'field_name': 'Microscope-Vendor',\n",
       "     'metadata_field': None},\n",
       "    {'id': 14,\n",
       "     'field_value': 'MFP3D',\n",
       "     'field_name': 'Microscope-Model',\n",
       "     'metadata_field': None}]},\n",
       "  {'id': 19,\n",
       "   'created_at': '2022-04-25T22:31:07Z',\n",
       "   'name': 'My new dataset with nested metadata',\n",
       "   'dataset_files': [{'id': 50,\n",
       "     'name': 'AFM_Topography.PNG',\n",
       "     'file_length': 201287,\n",
       "     'file_type': '',\n",
       "     'created_at': '2022-04-25 22:33:53 UTC',\n",
       "     'relative_path': '',\n",
       "     'is_directory': False},\n",
       "    {'id': 51,\n",
       "     'name': 'measurement_configuration.txt',\n",
       "     'file_length': 1172,\n",
       "     'file_type': '',\n",
       "     'created_at': '2022-04-25 22:34:01 UTC',\n",
       "     'relative_path': 'foo/bar',\n",
       "     'is_directory': False},\n",
       "    {'id': 52,\n",
       "     'name': 'foo',\n",
       "     'file_length': None,\n",
       "     'file_type': None,\n",
       "     'created_at': '2022-04-25 22:34:01 UTC',\n",
       "     'relative_path': '',\n",
       "     'is_directory': True},\n",
       "    {'id': 53,\n",
       "     'name': 'bar',\n",
       "     'file_length': None,\n",
       "     'file_type': None,\n",
       "     'created_at': '2022-04-25 22:34:01 UTC',\n",
       "     'relative_path': 'foo',\n",
       "     'is_directory': True}],\n",
       "   'metadata_field_values': [{'id': 15,\n",
       "     'field_value': 'PZT',\n",
       "     'field_name': 'Sample',\n",
       "     'metadata_field': None},\n",
       "    {'id': 16,\n",
       "     'field_value': 'Asylum Research',\n",
       "     'field_name': 'Microscope-Vendor',\n",
       "     'metadata_field': None},\n",
       "    {'id': 17,\n",
       "     'field_value': 'MFP3D',\n",
       "     'field_name': 'Microscope-Model',\n",
       "     'metadata_field': None},\n",
       "    {'id': 18,\n",
       "     'field_value': '373',\n",
       "     'field_name': 'Temperature',\n",
       "     'metadata_field': None}]}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.dataset_search(\"nested\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the response to get the dataset of interest for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset_id = response['results'][1]['id']\n",
    "dset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. View this Dataset:\n",
    "This view shows both the files and metadata contained in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 19,\n",
       " 'name': 'My new dataset with nested metadata',\n",
       " 'creator': {'id': 2, 'name': 'Suhas Somnath'},\n",
       " 'dataset_files': [{'id': 50,\n",
       "   'name': 'AFM_Topography.PNG',\n",
       "   'file_length': 201287,\n",
       "   'file_type': '',\n",
       "   'created_at': '2022-04-25 22:33:53 UTC',\n",
       "   'relative_path': '',\n",
       "   'is_directory': False},\n",
       "  {'id': 51,\n",
       "   'name': 'measurement_configuration.txt',\n",
       "   'file_length': 1172,\n",
       "   'file_type': '',\n",
       "   'created_at': '2022-04-25 22:34:01 UTC',\n",
       "   'relative_path': 'foo/bar',\n",
       "   'is_directory': False},\n",
       "  {'id': 52,\n",
       "   'name': 'foo',\n",
       "   'file_length': None,\n",
       "   'file_type': None,\n",
       "   'created_at': '2022-04-25 22:34:01 UTC',\n",
       "   'relative_path': '',\n",
       "   'is_directory': True},\n",
       "  {'id': 53,\n",
       "   'name': 'bar',\n",
       "   'file_length': None,\n",
       "   'file_type': None,\n",
       "   'created_at': '2022-04-25 22:34:01 UTC',\n",
       "   'relative_path': 'foo',\n",
       "   'is_directory': True}],\n",
       " 'instrument': None,\n",
       " 'metadata_field_values': [{'id': 15,\n",
       "   'field_value': 'PZT',\n",
       "   'field_name': 'Sample',\n",
       "   'metadata_field': None},\n",
       "  {'id': 16,\n",
       "   'field_value': 'Asylum Research',\n",
       "   'field_name': 'Microscope-Vendor',\n",
       "   'metadata_field': None},\n",
       "  {'id': 17,\n",
       "   'field_value': 'MFP3D',\n",
       "   'field_name': 'Microscope-Model',\n",
       "   'metadata_field': None},\n",
       "  {'id': 18,\n",
       "   'field_value': '373',\n",
       "   'field_name': 'Temperature',\n",
       "   'metadata_field': None}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = api.dataset_info(dset_id)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. View files uploaded via DataFlow:\n",
    "We're not using DataFlow here but just viewing the destination file system.\n",
    "\n",
    "Datasets are sorted by date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -hlt ~/dataflow/untitled_instrument/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be more than one dataset per day. Here we only have one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -hlt ~/dataflow/untitled_instrument/2022-04-06/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the root directory of the dataset we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -hlt ~/dataflow/untitled_instrument/2022-04-06/135750_atomic_force_microscopy_scan_of_pzt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will very soon be able to specify root level metadata that will be stored in ``metadata.json``.\n",
    "\n",
    "We can also see the nested directories: ``foo/bar`` where we uploaded the second file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -hlt  ~/dataflow/untitled_instrument/2022-04-06/135750_atomic_force_microscopy_scan_of_pzt/foo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the inner most directory - ``bar``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -hlt ~/dataflow/untitled_instrument/2022-04-06/135750_atomic_force_microscopy_scan_of_pzt/foo/bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
